{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assign before UTS DeepLearning 3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPI9GVUcMM+Rt3zB5BUte9H"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RKDrO0wvBoOX","executionInfo":{"status":"ok","timestamp":1637588012399,"user_tz":-420,"elapsed":27740,"user":{"displayName":"Nicholas Evan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11340839917959249724"}}},"source":["import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Cpc9JbIC9yx"},"source":["from torch.utils.data import Dataset\n","\n","class IrisDataset(Dataset):\n","  def __init__(self, X, y):\n","    self.X = X\n","    self.y = y\n","\n","  def __getitem__(self, index):\n","    X = torch.Tensor(self.X[index])\n","    y = torch.LongTensor(self.y[index, None])\n","    \n","    return X, y\n","\n","  def __len__(self):\n","    return len(self.X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSMzvC1UEx13"},"source":["# open dataset from csv\n","dataset = pd.read_csv('Iris.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8J6Y2TCC-4_"},"source":["\n","\n","# transform labels to numerics\n","dataset.loc[dataset.Species=='Iris-setosa', 'Species'] = 0\n","dataset.loc[dataset.Species=='Iris-versicolor', 'Species'] = 1\n","dataset.loc[dataset.Species=='Iris-virginica', 'Species'] = 2\n","\n","# get the features and labels from the dataset\n","X = dataset[dataset.columns[0:4]].values\n","y = dataset.Species.values.astype(np.int64)\n","\n","# preprocessing with z-score normalization\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2OEdSNrDAjj"},"source":["from sklearn.model_selection import train_test_split\n","\n","train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n","\n","train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, \n","                                                      test_size=0.2)\n","\n","from torch.utils.data import DataLoader\n","\n","train_ds = IrisDataset(train_X, train_y)\n","train_loader = DataLoader(train_ds, batch_size=16, \n","                             shuffle=True, num_workers=0)\n","\n","valid_ds = IrisDataset(valid_X, valid_y)\n","valid_loader = DataLoader(valid_ds, batch_size=16, \n","                             shuffle=False, num_workers=0)\n","\n","test_ds = IrisDataset(test_X, test_y)\n","test_loader = DataLoader(test_ds, batch_size=16, \n","                            shuffle=False, num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKv2tVfjDCKp"},"source":["\n","class Net(nn.Module):\n","    # define nn\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(4, 100)\n","        self.bn1 = nn.BatchNorm1d(100)\n","        \n","        self.fc2 = nn.Linear(100, 100)\n","        self.bn2 = nn.BatchNorm1d(100)\n","        \n","        self.fc3 = nn.Linear(100, 3)\n","\n","    def forward(self, X):\n","        X1 = self.fc1(X)\n","        X2 = F.relu(X1)\n","        X2 = self.bn1(X2)\n","        X=X1+X2\n","\n","        X1 = self.fc2(X)\n","        X2 = F.relu(X1)\n","        X2 = self.bn2(X2)\n","        X=X2+X1\n","\n","        X1 = self.fc2(X)\n","        X2 = F.relu(X1)\n","        X2 = self.bn2(X2)\n","        X = X1+X2\n","\n","        X = self.fc3(X)\n","\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X5y8wdvIbTB"},"source":["net =Net()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(net.parameters(),lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJGxsBRKDC_X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604919444726,"user_tz":-420,"elapsed":13574,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"87a7efcb-4e1c-4c19-8cf2-286a36c40cbb"},"source":["epochs = 300\n"," \n","train_mean_losses = []\n","valid_mean_losses = []\n","\n","valid_best_loss = np.inf\n","\n","\n","for i in range(epochs):  \n","    #===============================================================\n","    # training \n","    train_losses = []\n","    \n","    print(\"=========================================================\")\n","    print(\"Epoch {}\".format(i))\n","    \n","    for iteration, batch_data in enumerate(train_loader):\n","        X_batch, y_batch = batch_data\n","\n","        optimizer.zero_grad()\n","        \n","        out = net(X_batch)\n","        loss = criterion(out, y_batch.squeeze())\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        train_losses.append(loss)\n","    \n","    train_mean_loss = torch.mean(torch.stack(train_losses))\n","    print('training loss: {:10.8f}'.format(train_mean_loss))\n","    \n","    train_mean_losses.append(train_mean_loss)\n","    \n","    #===============================================================\n","    # validation\n","    valid_losses = []\n","    with torch.set_grad_enabled(False):\n","        for iteration, batch_data in enumerate(valid_loader):\n","            X_batch, y_batch = batch_data\n","\n","            out = net(X_batch)\n","            loss = criterion(out, y_batch.squeeze())\n","            valid_losses.append(loss)\n","            \n","        valid_mean_loss = torch.mean(torch.stack(valid_losses))\n","        print('validation loss: {:10.8f}'.format(valid_mean_loss))\n","        \n","        valid_mean_losses.append(valid_mean_loss)\n","        \n","        if valid_mean_loss.cpu().numpy()[()] < valid_best_loss:\n","            valid_best_loss = valid_mean_loss\n","            torch.save(net.state_dict(), \"best_model.pth\")\n","            best_epoch = i\n","    #==============================================================="],"execution_count":null,"outputs":[{"output_type":"stream","text":["=========================================================\n","Epoch 0\n","training loss: 0.60798520\n","validation loss: 0.34370989\n","=========================================================\n","Epoch 1\n","training loss: 0.17894566\n","validation loss: 0.25093928\n","=========================================================\n","Epoch 2\n","training loss: 0.08979040\n","validation loss: 0.19124517\n","=========================================================\n","Epoch 3\n","training loss: 0.05835341\n","validation loss: 0.16909632\n","=========================================================\n","Epoch 4\n","training loss: 0.09878594\n","validation loss: 0.16789880\n","=========================================================\n","Epoch 5\n","training loss: 0.05588047\n","validation loss: 0.15857455\n","=========================================================\n","Epoch 6\n","training loss: 0.04816940\n","validation loss: 0.14585021\n","=========================================================\n","Epoch 7\n","training loss: 0.12809481\n","validation loss: 0.21652500\n","=========================================================\n","Epoch 8\n","training loss: 0.08568364\n","validation loss: 0.13248311\n","=========================================================\n","Epoch 9\n","training loss: 0.02908302\n","validation loss: 0.12303453\n","=========================================================\n","Epoch 10\n","training loss: 0.06232971\n","validation loss: 0.11341258\n","=========================================================\n","Epoch 11\n","training loss: 0.02439443\n","validation loss: 0.10549522\n","=========================================================\n","Epoch 12\n","training loss: 0.01891101\n","validation loss: 0.09845359\n","=========================================================\n","Epoch 13\n","training loss: 0.02913864\n","validation loss: 0.08717903\n","=========================================================\n","Epoch 14\n","training loss: 0.02971397\n","validation loss: 0.07992689\n","=========================================================\n","Epoch 15\n","training loss: 0.03198232\n","validation loss: 0.07697501\n","=========================================================\n","Epoch 16\n","training loss: 0.05224733\n","validation loss: 0.07352113\n","=========================================================\n","Epoch 17\n","training loss: 0.03160337\n","validation loss: 0.07277719\n","=========================================================\n","Epoch 18\n","training loss: 0.02520175\n","validation loss: 0.07163432\n","=========================================================\n","Epoch 19\n","training loss: 0.03467085\n","validation loss: 0.07001565\n","=========================================================\n","Epoch 20\n","training loss: 0.02621097\n","validation loss: 0.06790269\n","=========================================================\n","Epoch 21\n","training loss: 0.06790855\n","validation loss: 0.07931224\n","=========================================================\n","Epoch 22\n","training loss: 0.01912124\n","validation loss: 0.07296491\n","=========================================================\n","Epoch 23\n","training loss: 0.01789789\n","validation loss: 0.06908833\n","=========================================================\n","Epoch 24\n","training loss: 0.01834585\n","validation loss: 0.06590667\n","=========================================================\n","Epoch 25\n","training loss: 0.12061606\n","validation loss: 0.06629313\n","=========================================================\n","Epoch 26\n","training loss: 0.01033439\n","validation loss: 0.06452250\n","=========================================================\n","Epoch 27\n","training loss: 0.01318065\n","validation loss: 0.05860241\n","=========================================================\n","Epoch 28\n","training loss: 0.01523819\n","validation loss: 0.05543681\n","=========================================================\n","Epoch 29\n","training loss: 0.01830196\n","validation loss: 0.05430506\n","=========================================================\n","Epoch 30\n","training loss: 0.01576103\n","validation loss: 0.04873910\n","=========================================================\n","Epoch 31\n","training loss: 0.01480186\n","validation loss: 0.04654314\n","=========================================================\n","Epoch 32\n","training loss: 0.02104991\n","validation loss: 0.04340867\n","=========================================================\n","Epoch 33\n","training loss: 0.00594238\n","validation loss: 0.04271279\n","=========================================================\n","Epoch 34\n","training loss: 0.01846553\n","validation loss: 0.04085079\n","=========================================================\n","Epoch 35\n","training loss: 0.05163643\n","validation loss: 0.04179212\n","=========================================================\n","Epoch 36\n","training loss: 0.01622722\n","validation loss: 0.03945645\n","=========================================================\n","Epoch 37\n","training loss: 0.02190184\n","validation loss: 0.03599570\n","=========================================================\n","Epoch 38\n","training loss: 0.00948585\n","validation loss: 0.03637453\n","=========================================================\n","Epoch 39\n","training loss: 0.02349933\n","validation loss: 0.03722991\n","=========================================================\n","Epoch 40\n","training loss: 0.05600885\n","validation loss: 0.03669048\n","=========================================================\n","Epoch 41\n","training loss: 0.01343607\n","validation loss: 0.03515334\n","=========================================================\n","Epoch 42\n","training loss: 0.00781110\n","validation loss: 0.03449365\n","=========================================================\n","Epoch 43\n","training loss: 0.03663625\n","validation loss: 0.03637476\n","=========================================================\n","Epoch 44\n","training loss: 0.01696165\n","validation loss: 0.03534913\n","=========================================================\n","Epoch 45\n","training loss: 0.02040814\n","validation loss: 0.03389141\n","=========================================================\n","Epoch 46\n","training loss: 0.01695632\n","validation loss: 0.03361456\n","=========================================================\n","Epoch 47\n","training loss: 0.01070873\n","validation loss: 0.03249588\n","=========================================================\n","Epoch 48\n","training loss: 0.00959392\n","validation loss: 0.03267181\n","=========================================================\n","Epoch 49\n","training loss: 0.00984579\n","validation loss: 0.03286797\n","=========================================================\n","Epoch 50\n","training loss: 0.00928138\n","validation loss: 0.03272613\n","=========================================================\n","Epoch 51\n","training loss: 0.02976877\n","validation loss: 0.03126253\n","=========================================================\n","Epoch 52\n","training loss: 0.07476953\n","validation loss: 0.02722534\n","=========================================================\n","Epoch 53\n","training loss: 0.00739041\n","validation loss: 0.02737411\n","=========================================================\n","Epoch 54\n","training loss: 0.05029284\n","validation loss: 0.02649267\n","=========================================================\n","Epoch 55\n","training loss: 0.01982374\n","validation loss: 0.02851038\n","=========================================================\n","Epoch 56\n","training loss: 0.00888203\n","validation loss: 0.02816558\n","=========================================================\n","Epoch 57\n","training loss: 0.00662539\n","validation loss: 0.02772416\n","=========================================================\n","Epoch 58\n","training loss: 0.02482424\n","validation loss: 0.03009435\n","=========================================================\n","Epoch 59\n","training loss: 0.01221862\n","validation loss: 0.02721999\n","=========================================================\n","Epoch 60\n","training loss: 0.01406191\n","validation loss: 0.02456624\n","=========================================================\n","Epoch 61\n","training loss: 0.00777541\n","validation loss: 0.02622684\n","=========================================================\n","Epoch 62\n","training loss: 0.01078090\n","validation loss: 0.02660768\n","=========================================================\n","Epoch 63\n","training loss: 0.00726366\n","validation loss: 0.02675621\n","=========================================================\n","Epoch 64\n","training loss: 0.01802931\n","validation loss: 0.02717903\n","=========================================================\n","Epoch 65\n","training loss: 0.00920782\n","validation loss: 0.02691475\n","=========================================================\n","Epoch 66\n","training loss: 0.01448176\n","validation loss: 0.02788810\n","=========================================================\n","Epoch 67\n","training loss: 0.01705813\n","validation loss: 0.02543675\n","=========================================================\n","Epoch 68\n","training loss: 0.00701427\n","validation loss: 0.02572562\n","=========================================================\n","Epoch 69\n","training loss: 0.00869543\n","validation loss: 0.02463872\n","=========================================================\n","Epoch 70\n","training loss: 0.03228591\n","validation loss: 0.02551875\n","=========================================================\n","Epoch 71\n","training loss: 0.00730990\n","validation loss: 0.02569790\n","=========================================================\n","Epoch 72\n","training loss: 0.02268774\n","validation loss: 0.02361450\n","=========================================================\n","Epoch 73\n","training loss: 0.00441859\n","validation loss: 0.02364509\n","=========================================================\n","Epoch 74\n","training loss: 0.01036060\n","validation loss: 0.02419113\n","=========================================================\n","Epoch 75\n","training loss: 0.00651577\n","validation loss: 0.02419056\n","=========================================================\n","Epoch 76\n","training loss: 0.00254522\n","validation loss: 0.02363152\n","=========================================================\n","Epoch 77\n","training loss: 0.00393023\n","validation loss: 0.02302701\n","=========================================================\n","Epoch 78\n","training loss: 0.01463004\n","validation loss: 0.02178161\n","=========================================================\n","Epoch 79\n","training loss: 0.02549544\n","validation loss: 0.02026256\n","=========================================================\n","Epoch 80\n","training loss: 0.00248248\n","validation loss: 0.02038561\n","=========================================================\n","Epoch 81\n","training loss: 0.01091234\n","validation loss: 0.01956264\n","=========================================================\n","Epoch 82\n","training loss: 0.00393331\n","validation loss: 0.01947970\n","=========================================================\n","Epoch 83\n","training loss: 0.00466800\n","validation loss: 0.01967173\n","=========================================================\n","Epoch 84\n","training loss: 0.08673079\n","validation loss: 0.03168031\n","=========================================================\n","Epoch 85\n","training loss: 0.00809902\n","validation loss: 0.02869906\n","=========================================================\n","Epoch 86\n","training loss: 0.01204548\n","validation loss: 0.02535744\n","=========================================================\n","Epoch 87\n","training loss: 0.00243838\n","validation loss: 0.02503262\n","=========================================================\n","Epoch 88\n","training loss: 0.05007871\n","validation loss: 0.02536652\n","=========================================================\n","Epoch 89\n","training loss: 0.00348308\n","validation loss: 0.02408779\n","=========================================================\n","Epoch 90\n","training loss: 0.00301087\n","validation loss: 0.02321433\n","=========================================================\n","Epoch 91\n","training loss: 0.00321478\n","validation loss: 0.02293964\n","=========================================================\n","Epoch 92\n","training loss: 0.01778810\n","validation loss: 0.02211014\n","=========================================================\n","Epoch 93\n","training loss: 0.00659128\n","validation loss: 0.02082132\n","=========================================================\n","Epoch 94\n","training loss: 0.00387467\n","validation loss: 0.02057660\n","=========================================================\n","Epoch 95\n","training loss: 0.00292432\n","validation loss: 0.02022121\n","=========================================================\n","Epoch 96\n","training loss: 0.00491244\n","validation loss: 0.02015400\n","=========================================================\n","Epoch 97\n","training loss: 0.00652958\n","validation loss: 0.01953143\n","=========================================================\n","Epoch 98\n","training loss: 0.01479302\n","validation loss: 0.01990932\n","=========================================================\n","Epoch 99\n","training loss: 0.01489297\n","validation loss: 0.01931144\n","=========================================================\n","Epoch 100\n","training loss: 0.00629409\n","validation loss: 0.01884006\n","=========================================================\n","Epoch 101\n","training loss: 0.00521189\n","validation loss: 0.01900703\n","=========================================================\n","Epoch 102\n","training loss: 0.00162062\n","validation loss: 0.01884701\n","=========================================================\n","Epoch 103\n","training loss: 0.03071641\n","validation loss: 0.01899893\n","=========================================================\n","Epoch 104\n","training loss: 0.01694053\n","validation loss: 0.02051417\n","=========================================================\n","Epoch 105\n","training loss: 0.02858975\n","validation loss: 0.01878598\n","=========================================================\n","Epoch 106\n","training loss: 0.01607379\n","validation loss: 0.01643714\n","=========================================================\n","Epoch 107\n","training loss: 0.00314154\n","validation loss: 0.01633355\n","=========================================================\n","Epoch 108\n","training loss: 0.01065816\n","validation loss: 0.01600788\n","=========================================================\n","Epoch 109\n","training loss: 0.02399966\n","validation loss: 0.01663583\n","=========================================================\n","Epoch 110\n","training loss: 0.00158976\n","validation loss: 0.01669291\n","=========================================================\n","Epoch 111\n","training loss: 0.00248256\n","validation loss: 0.01659603\n","=========================================================\n","Epoch 112\n","training loss: 0.00354126\n","validation loss: 0.01624896\n","=========================================================\n","Epoch 113\n","training loss: 0.00838204\n","validation loss: 0.01466184\n","=========================================================\n","Epoch 114\n","training loss: 0.00208313\n","validation loss: 0.01475297\n","=========================================================\n","Epoch 115\n","training loss: 0.00118747\n","validation loss: 0.01476483\n","=========================================================\n","Epoch 116\n","training loss: 0.00204695\n","validation loss: 0.01448251\n","=========================================================\n","Epoch 117\n","training loss: 0.00274378\n","validation loss: 0.01460749\n","=========================================================\n","Epoch 118\n","training loss: 0.05002144\n","validation loss: 0.01987164\n","=========================================================\n","Epoch 119\n","training loss: 0.02072609\n","validation loss: 0.02063120\n","=========================================================\n","Epoch 120\n","training loss: 0.00303866\n","validation loss: 0.01977756\n","=========================================================\n","Epoch 121\n","training loss: 0.00313666\n","validation loss: 0.01922733\n","=========================================================\n","Epoch 122\n","training loss: 0.00909765\n","validation loss: 0.01715364\n","=========================================================\n","Epoch 123\n","training loss: 0.00531564\n","validation loss: 0.01607691\n","=========================================================\n","Epoch 124\n","training loss: 0.00291499\n","validation loss: 0.01534051\n","=========================================================\n","Epoch 125\n","training loss: 0.00726424\n","validation loss: 0.01544339\n","=========================================================\n","Epoch 126\n","training loss: 0.01344726\n","validation loss: 0.01578021\n","=========================================================\n","Epoch 127\n","training loss: 0.00807506\n","validation loss: 0.01473847\n","=========================================================\n","Epoch 128\n","training loss: 0.00692921\n","validation loss: 0.01447556\n","=========================================================\n","Epoch 129\n","training loss: 0.00748326\n","validation loss: 0.01525988\n","=========================================================\n","Epoch 130\n","training loss: 0.00407543\n","validation loss: 0.01412578\n","=========================================================\n","Epoch 131\n","training loss: 0.08512252\n","validation loss: 0.01813875\n","=========================================================\n","Epoch 132\n","training loss: 0.06052496\n","validation loss: 0.02528241\n","=========================================================\n","Epoch 133\n","training loss: 0.00935244\n","validation loss: 0.02303332\n","=========================================================\n","Epoch 134\n","training loss: 0.01950578\n","validation loss: 0.03123275\n","=========================================================\n","Epoch 135\n","training loss: 0.04225039\n","validation loss: 0.01825502\n","=========================================================\n","Epoch 136\n","training loss: 0.00455912\n","validation loss: 0.01706873\n","=========================================================\n","Epoch 137\n","training loss: 0.00233012\n","validation loss: 0.01712197\n","=========================================================\n","Epoch 138\n","training loss: 0.00142927\n","validation loss: 0.01709455\n","=========================================================\n","Epoch 139\n","training loss: 0.00129145\n","validation loss: 0.01701357\n","=========================================================\n","Epoch 140\n","training loss: 0.00546904\n","validation loss: 0.01658242\n","=========================================================\n","Epoch 141\n","training loss: 0.04059317\n","validation loss: 0.01775397\n","=========================================================\n","Epoch 142\n","training loss: 0.02697260\n","validation loss: 0.01628394\n","=========================================================\n","Epoch 143\n","training loss: 0.00353111\n","validation loss: 0.01552403\n","=========================================================\n","Epoch 144\n","training loss: 0.00329306\n","validation loss: 0.01542612\n","=========================================================\n","Epoch 145\n","training loss: 0.00699986\n","validation loss: 0.01575287\n","=========================================================\n","Epoch 146\n","training loss: 0.02512042\n","validation loss: 0.01398992\n","=========================================================\n","Epoch 147\n","training loss: 0.02989855\n","validation loss: 0.01565269\n","=========================================================\n","Epoch 148\n","training loss: 0.00268339\n","validation loss: 0.01540661\n","=========================================================\n","Epoch 149\n","training loss: 0.00608197\n","validation loss: 0.01504278\n","=========================================================\n","Epoch 150\n","training loss: 0.00346977\n","validation loss: 0.01525285\n","=========================================================\n","Epoch 151\n","training loss: 0.00687127\n","validation loss: 0.01582208\n","=========================================================\n","Epoch 152\n","training loss: 0.02950553\n","validation loss: 0.01543550\n","=========================================================\n","Epoch 153\n","training loss: 0.00510161\n","validation loss: 0.01524369\n","=========================================================\n","Epoch 154\n","training loss: 0.02118457\n","validation loss: 0.02012431\n","=========================================================\n","Epoch 155\n","training loss: 0.00428710\n","validation loss: 0.01827092\n","=========================================================\n","Epoch 156\n","training loss: 0.00497791\n","validation loss: 0.01717097\n","=========================================================\n","Epoch 157\n","training loss: 0.04228638\n","validation loss: 0.01621942\n","=========================================================\n","Epoch 158\n","training loss: 0.02166981\n","validation loss: 0.01378462\n","=========================================================\n","Epoch 159\n","training loss: 0.00175857\n","validation loss: 0.01363793\n","=========================================================\n","Epoch 160\n","training loss: 0.00476086\n","validation loss: 0.01351695\n","=========================================================\n","Epoch 161\n","training loss: 0.00592750\n","validation loss: 0.01307902\n","=========================================================\n","Epoch 162\n","training loss: 0.00377231\n","validation loss: 0.01308630\n","=========================================================\n","Epoch 163\n","training loss: 0.02032016\n","validation loss: 0.01388191\n","=========================================================\n","Epoch 164\n","training loss: 0.00548809\n","validation loss: 0.01360450\n","=========================================================\n","Epoch 165\n","training loss: 0.00275461\n","validation loss: 0.01305543\n","=========================================================\n","Epoch 166\n","training loss: 0.00296365\n","validation loss: 0.01269649\n","=========================================================\n","Epoch 167\n","training loss: 0.00593771\n","validation loss: 0.01222236\n","=========================================================\n","Epoch 168\n","training loss: 0.00174778\n","validation loss: 0.01214348\n","=========================================================\n","Epoch 169\n","training loss: 0.00099015\n","validation loss: 0.01207797\n","=========================================================\n","Epoch 170\n","training loss: 0.01711423\n","validation loss: 0.01206527\n","=========================================================\n","Epoch 171\n","training loss: 0.00570980\n","validation loss: 0.01154214\n","=========================================================\n","Epoch 172\n","training loss: 0.00589198\n","validation loss: 0.01116238\n","=========================================================\n","Epoch 173\n","training loss: 0.00278494\n","validation loss: 0.01108277\n","=========================================================\n","Epoch 174\n","training loss: 0.00197033\n","validation loss: 0.01098068\n","=========================================================\n","Epoch 175\n","training loss: 0.00799946\n","validation loss: 0.01106437\n","=========================================================\n","Epoch 176\n","training loss: 0.00243519\n","validation loss: 0.01109080\n","=========================================================\n","Epoch 177\n","training loss: 0.01243080\n","validation loss: 0.01128352\n","=========================================================\n","Epoch 178\n","training loss: 0.00214875\n","validation loss: 0.01124611\n","=========================================================\n","Epoch 179\n","training loss: 0.00121591\n","validation loss: 0.01113573\n","=========================================================\n","Epoch 180\n","training loss: 0.00144440\n","validation loss: 0.01098553\n","=========================================================\n","Epoch 181\n","training loss: 0.00259347\n","validation loss: 0.01109072\n","=========================================================\n","Epoch 182\n","training loss: 0.00260487\n","validation loss: 0.01094014\n","=========================================================\n","Epoch 183\n","training loss: 0.00733282\n","validation loss: 0.01120248\n","=========================================================\n","Epoch 184\n","training loss: 0.04351649\n","validation loss: 0.01509975\n","=========================================================\n","Epoch 185\n","training loss: 0.00096127\n","validation loss: 0.01482802\n","=========================================================\n","Epoch 186\n","training loss: 0.06293847\n","validation loss: 0.01079470\n","=========================================================\n","Epoch 187\n","training loss: 0.00268882\n","validation loss: 0.01064927\n","=========================================================\n","Epoch 188\n","training loss: 0.00232500\n","validation loss: 0.01060993\n","=========================================================\n","Epoch 189\n","training loss: 0.00690078\n","validation loss: 0.01038733\n","=========================================================\n","Epoch 190\n","training loss: 0.00116711\n","validation loss: 0.01048821\n","=========================================================\n","Epoch 191\n","training loss: 0.01706674\n","validation loss: 0.01165879\n","=========================================================\n","Epoch 192\n","training loss: 0.00188836\n","validation loss: 0.01182079\n","=========================================================\n","Epoch 193\n","training loss: 0.00333072\n","validation loss: 0.01171760\n","=========================================================\n","Epoch 194\n","training loss: 0.00204730\n","validation loss: 0.01158850\n","=========================================================\n","Epoch 195\n","training loss: 0.00628091\n","validation loss: 0.01159833\n","=========================================================\n","Epoch 196\n","training loss: 0.00326649\n","validation loss: 0.01133387\n","=========================================================\n","Epoch 197\n","training loss: 0.00365933\n","validation loss: 0.01134515\n","=========================================================\n","Epoch 198\n","training loss: 0.01927003\n","validation loss: 0.01155534\n","=========================================================\n","Epoch 199\n","training loss: 0.00530347\n","validation loss: 0.01167548\n","=========================================================\n","Epoch 200\n","training loss: 0.00124817\n","validation loss: 0.01169481\n","=========================================================\n","Epoch 201\n","training loss: 0.00893875\n","validation loss: 0.01208618\n","=========================================================\n","Epoch 202\n","training loss: 0.00504665\n","validation loss: 0.01080976\n","=========================================================\n","Epoch 203\n","training loss: 0.00346481\n","validation loss: 0.01032950\n","=========================================================\n","Epoch 204\n","training loss: 0.01518578\n","validation loss: 0.01286892\n","=========================================================\n","Epoch 205\n","training loss: 0.03228039\n","validation loss: 0.01490284\n","=========================================================\n","Epoch 206\n","training loss: 0.00356428\n","validation loss: 0.01379094\n","=========================================================\n","Epoch 207\n","training loss: 0.00240902\n","validation loss: 0.01337319\n","=========================================================\n","Epoch 208\n","training loss: 0.00220704\n","validation loss: 0.01326485\n","=========================================================\n","Epoch 209\n","training loss: 0.00280921\n","validation loss: 0.01275172\n","=========================================================\n","Epoch 210\n","training loss: 0.05991693\n","validation loss: 0.03305519\n","=========================================================\n","Epoch 211\n","training loss: 0.01615734\n","validation loss: 0.01721861\n","=========================================================\n","Epoch 212\n","training loss: 0.00171680\n","validation loss: 0.01612008\n","=========================================================\n","Epoch 213\n","training loss: 0.00415439\n","validation loss: 0.01489242\n","=========================================================\n","Epoch 214\n","training loss: 0.04746762\n","validation loss: 0.01776326\n","=========================================================\n","Epoch 215\n","training loss: 0.00045546\n","validation loss: 0.01756632\n","=========================================================\n","Epoch 216\n","training loss: 0.02772599\n","validation loss: 0.02232832\n","=========================================================\n","Epoch 217\n","training loss: 0.00066431\n","validation loss: 0.02175556\n","=========================================================\n","Epoch 218\n","training loss: 0.01683340\n","validation loss: 0.01553172\n","=========================================================\n","Epoch 219\n","training loss: 0.00276606\n","validation loss: 0.01490628\n","=========================================================\n","Epoch 220\n","training loss: 0.00666251\n","validation loss: 0.01226302\n","=========================================================\n","Epoch 221\n","training loss: 0.00803057\n","validation loss: 0.01254396\n","=========================================================\n","Epoch 222\n","training loss: 0.00238073\n","validation loss: 0.01258838\n","=========================================================\n","Epoch 223\n","training loss: 0.00150357\n","validation loss: 0.01240448\n","=========================================================\n","Epoch 224\n","training loss: 0.12970474\n","validation loss: 0.04933788\n","=========================================================\n","Epoch 225\n","training loss: 0.05194368\n","validation loss: 0.01250968\n","=========================================================\n","Epoch 226\n","training loss: 0.00334011\n","validation loss: 0.01242344\n","=========================================================\n","Epoch 227\n","training loss: 0.00700196\n","validation loss: 0.01253614\n","=========================================================\n","Epoch 228\n","training loss: 0.00104116\n","validation loss: 0.01251227\n","=========================================================\n","Epoch 229\n","training loss: 0.00662929\n","validation loss: 0.01217213\n","=========================================================\n","Epoch 230\n","training loss: 0.05403427\n","validation loss: 0.01439189\n","=========================================================\n","Epoch 231\n","training loss: 0.00162063\n","validation loss: 0.01399522\n","=========================================================\n","Epoch 232\n","training loss: 0.00172557\n","validation loss: 0.01387154\n","=========================================================\n","Epoch 233\n","training loss: 0.00203496\n","validation loss: 0.01379492\n","=========================================================\n","Epoch 234\n","training loss: 0.00068290\n","validation loss: 0.01370433\n","=========================================================\n","Epoch 235\n","training loss: 0.00179688\n","validation loss: 0.01357244\n","=========================================================\n","Epoch 236\n","training loss: 0.00663754\n","validation loss: 0.01346972\n","=========================================================\n","Epoch 237\n","training loss: 0.00346551\n","validation loss: 0.01365973\n","=========================================================\n","Epoch 238\n","training loss: 0.03952470\n","validation loss: 0.01764001\n","=========================================================\n","Epoch 239\n","training loss: 0.00381126\n","validation loss: 0.01664364\n","=========================================================\n","Epoch 240\n","training loss: 0.02393163\n","validation loss: 0.01743725\n","=========================================================\n","Epoch 241\n","training loss: 0.00618607\n","validation loss: 0.01722087\n","=========================================================\n","Epoch 242\n","training loss: 0.00343389\n","validation loss: 0.01567612\n","=========================================================\n","Epoch 243\n","training loss: 0.01122694\n","validation loss: 0.01242008\n","=========================================================\n","Epoch 244\n","training loss: 0.00414157\n","validation loss: 0.01212780\n","=========================================================\n","Epoch 245\n","training loss: 0.07285082\n","validation loss: 0.01901210\n","=========================================================\n","Epoch 246\n","training loss: 0.00241859\n","validation loss: 0.01867040\n","=========================================================\n","Epoch 247\n","training loss: 0.00144389\n","validation loss: 0.01847518\n","=========================================================\n","Epoch 248\n","training loss: 0.00610433\n","validation loss: 0.01761463\n","=========================================================\n","Epoch 249\n","training loss: 0.00513237\n","validation loss: 0.01579744\n","=========================================================\n","Epoch 250\n","training loss: 0.00106727\n","validation loss: 0.01595534\n","=========================================================\n","Epoch 251\n","training loss: 0.00186503\n","validation loss: 0.01590300\n","=========================================================\n","Epoch 252\n","training loss: 0.00116994\n","validation loss: 0.01528137\n","=========================================================\n","Epoch 253\n","training loss: 0.00279712\n","validation loss: 0.01498053\n","=========================================================\n","Epoch 254\n","training loss: 0.00199278\n","validation loss: 0.01482742\n","=========================================================\n","Epoch 255\n","training loss: 0.01047514\n","validation loss: 0.01619958\n","=========================================================\n","Epoch 256\n","training loss: 0.00284398\n","validation loss: 0.01559664\n","=========================================================\n","Epoch 257\n","training loss: 0.00626733\n","validation loss: 0.01412179\n","=========================================================\n","Epoch 258\n","training loss: 0.00549786\n","validation loss: 0.01232436\n","=========================================================\n","Epoch 259\n","training loss: 0.00088327\n","validation loss: 0.01221153\n","=========================================================\n","Epoch 260\n","training loss: 0.00064749\n","validation loss: 0.01206743\n","=========================================================\n","Epoch 261\n","training loss: 0.00043143\n","validation loss: 0.01201690\n","=========================================================\n","Epoch 262\n","training loss: 0.00664279\n","validation loss: 0.01210395\n","=========================================================\n","Epoch 263\n","training loss: 0.00309245\n","validation loss: 0.01155825\n","=========================================================\n","Epoch 264\n","training loss: 0.00066947\n","validation loss: 0.01151207\n","=========================================================\n","Epoch 265\n","training loss: 0.00259520\n","validation loss: 0.01182840\n","=========================================================\n","Epoch 266\n","training loss: 0.01470812\n","validation loss: 0.01264640\n","=========================================================\n","Epoch 267\n","training loss: 0.00180634\n","validation loss: 0.01263369\n","=========================================================\n","Epoch 268\n","training loss: 0.00274314\n","validation loss: 0.01250627\n","=========================================================\n","Epoch 269\n","training loss: 0.00440442\n","validation loss: 0.01093260\n","=========================================================\n","Epoch 270\n","training loss: 0.00118168\n","validation loss: 0.01099074\n","=========================================================\n","Epoch 271\n","training loss: 0.00092287\n","validation loss: 0.01096270\n","=========================================================\n","Epoch 272\n","training loss: 0.00102746\n","validation loss: 0.01093311\n","=========================================================\n","Epoch 273\n","training loss: 0.06541397\n","validation loss: 0.01466409\n","=========================================================\n","Epoch 274\n","training loss: 0.00898412\n","validation loss: 0.01727889\n","=========================================================\n","Epoch 275\n","training loss: 0.00683668\n","validation loss: 0.01764153\n","=========================================================\n","Epoch 276\n","training loss: 0.00612543\n","validation loss: 0.01841171\n","=========================================================\n","Epoch 277\n","training loss: 0.00249905\n","validation loss: 0.01816659\n","=========================================================\n","Epoch 278\n","training loss: 0.00133127\n","validation loss: 0.01760308\n","=========================================================\n","Epoch 279\n","training loss: 0.01024877\n","validation loss: 0.01681944\n","=========================================================\n","Epoch 280\n","training loss: 0.01017895\n","validation loss: 0.01734105\n","=========================================================\n","Epoch 281\n","training loss: 0.00638440\n","validation loss: 0.01705090\n","=========================================================\n","Epoch 282\n","training loss: 0.01229673\n","validation loss: 0.01338768\n","=========================================================\n","Epoch 283\n","training loss: 0.00250268\n","validation loss: 0.01349497\n","=========================================================\n","Epoch 284\n","training loss: 0.00702289\n","validation loss: 0.01459529\n","=========================================================\n","Epoch 285\n","training loss: 0.00152911\n","validation loss: 0.01411154\n","=========================================================\n","Epoch 286\n","training loss: 0.00431913\n","validation loss: 0.01316092\n","=========================================================\n","Epoch 287\n","training loss: 0.00034330\n","validation loss: 0.01314163\n","=========================================================\n","Epoch 288\n","training loss: 0.00060314\n","validation loss: 0.01315018\n","=========================================================\n","Epoch 289\n","training loss: 0.00103571\n","validation loss: 0.01337352\n","=========================================================\n","Epoch 290\n","training loss: 0.00114545\n","validation loss: 0.01325997\n","=========================================================\n","Epoch 291\n","training loss: 0.00225161\n","validation loss: 0.01392435\n","=========================================================\n","Epoch 292\n","training loss: 0.00237809\n","validation loss: 0.01380855\n","=========================================================\n","Epoch 293\n","training loss: 0.00098143\n","validation loss: 0.01368941\n","=========================================================\n","Epoch 294\n","training loss: 0.00469661\n","validation loss: 0.01522908\n","=========================================================\n","Epoch 295\n","training loss: 0.00351385\n","validation loss: 0.01431408\n","=========================================================\n","Epoch 296\n","training loss: 0.00098463\n","validation loss: 0.01397204\n","=========================================================\n","Epoch 297\n","training loss: 0.00054491\n","validation loss: 0.01390461\n","=========================================================\n","Epoch 298\n","training loss: 0.00146275\n","validation loss: 0.01412616\n","=========================================================\n","Epoch 299\n","training loss: 0.21206318\n","validation loss: 0.03511997\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wbiaPi8tOyy1","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1604919444727,"user_tz":-420,"elapsed":13571,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"866dd28b-5bd8-475e-92ed-93dbfa6b4f30"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1,epochs+1), train_mean_losses)\n","plt.plot(range(1,epochs+1), valid_mean_losses)\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.title('Train and Validation Loss Plot')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"6Fh94rmUSPb9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604919445225,"user_tz":-420,"elapsed":14065,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"157df97f-f3c3-4a65-f8b3-94e2a3483358"},"source":["inp = [\n","    [0.17, 0.95, 0.64, -0.7, -0.81, -0.3, 0.06, -0.05],\n","    [0.79, 0.03, 0.69, 0.43, 0.86, -0.08, 0.54, -0.37],\n","    [0.81, 0.03, 0.4, 0.78, -0.34, 0.08, -0.73, 0.74],\n","    [0.59, -0.89, 0.39, 0.22, -0.56, 0.22, 0.1, 0.6],\n","    [0.17, 0.34, 0.47, 0.67, -0.24, 0, 0.33, -0.94],\n","    [0.58, 0.68, 0.67, -0.59, 0.57, 0.39, -0.66, 0.53],\n","    [-0.79, 0.54, 0.47, 0.94, 0.64, 0.72, -0.73, 0.03],\n","    [-0.64, 0.38, 0.92, -0.85, 0.71, 0.14, 0.38, 0.21]\n","]\n","\n","inp = np.array(inp)\n","\n","kernel =[         \n","[-0.3,-0.85,0.06],\n","[-0.95,0.21,0.36],\n","[0.22,-0.07,0.59],\n","]\n","kernel=np.array(kernel)\n","\n","\n","def convolve2D(image, kernel, padding=0, strides=1):\n","    # Cross Correlation\n","    kernel = np.flipud(np.fliplr(kernel))\n","\n","    # Gather Shapes of Kernel + Image + Padding\n","    xKernShape = kernel.shape[0]\n","    yKernShape = kernel.shape[1]\n","    xImgShape = image.shape[0]\n","    yImgShape = image.shape[0]\n","\n","    # Shape of Output Convolution\n","    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n","    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n","    output = np.zeros((xOutput, yOutput))\n","\n","    # Apply Equal Padding to All Sides\n","    if padding != 0:\n","        imagePadded = np.zeros((image.shape[0] + padding*2, image.shape[1] + padding*2))\n","        imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image\n","        print(imagePadded)\n","    else:\n","        imagePadded = image\n","\n","    # Iterate through image\n","    for y in range(image.shape[1]):\n","        # Exit Convolution\n","        if y > image.shape[1] - yKernShape:\n","            break\n","        # Only Convolve if y has gone down by the specified Strides\n","        if y % strides == 0:\n","            for x in range(image.shape[0]):\n","                # Go to next row once kernel is out of bounds\n","                if x > image.shape[0] - xKernShape:\n","                    break\n","                try:\n","                    # Only Convolve if x has moved by the specified Strides\n","                    if x % strides == 0:\n","                        output[x, y] = (kernel * imagePadded[x: x + xKernShape, y: y + yKernShape]).sum()\n","                except:\n","                    break\n","\n","    return output\n","\n","\n","\n","\n","\n","output = convolve2D(inp, kernel)\n","  \n","print(output)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-0.2871 -0.4633 -0.7669  0.3009 -0.5333  0.6472]\n"," [ 1.2086 -1.0331  1.2014  0.7325  0.9691 -1.2457]\n"," [-0.201  -0.8663  0.3559  0.4984 -0.7306 -0.2072]\n"," [-0.5619 -1.2711  1.0001 -0.2287 -0.823   1.6425]\n"," [-0.7524  0.612  -1.2094 -0.7547  0.4905 -0.0769]\n"," [-0.8129 -0.8796  0.8853 -1.2099  1.0485  0.0927]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JtaoHMEv_K_4"},"source":["source https://medium.com/analytics-vidhya/2d-convolution-using-python-numpy-43442ff5f381\n"]}]}