{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UAS_NLP","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNmyeXIN4334CI4NgppTr7L"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_4_sHTdjFQgD","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593598895090,"user_tz":-420,"elapsed":4195,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"a8cb5b48-c314-4fa8-afc6-02b2fd87d910"},"source":["pip install nltk\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4QSpaWzuFAxK"},"source":["import nltk\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGoLO92Y1BAE","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"ok","timestamp":1593598895618,"user_tz":-420,"elapsed":4700,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"548b3eef-5d10-459f-9d02-26f9cc0d4590"},"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"OsuM5mc3mao4"},"source":["from nltk import punkt\n","from nltk.corpus import stopwords\n","import pandas as pd\n","ds = pd.read_csv('DataNLP.csv')\n","ds = ds.applymap(str.lower)\n","#remove all number\n","ds['Data'] = ds['Data'].str.replace(r'\\d+','')\n","#remove all punctuation\n","ds['Data'] = ds['Data'].str.replace('[^\\w\\s]','')\n","#Remove WhiteSpaces\n","ds['Data'] = ds['Data'].str.strip()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAw5a1WvdSnV"},"source":["We need to remove all the lowering the data helps to make the dataset become consistent. We also need to remove number and punctuation as it isn't relevant. Also we remove whitespaces to remove trailing and ending space."]},{"cell_type":"markdown","metadata":{"id":"obTGIhV40JW4"},"source":["Tokenization and Stopword removal"]},{"cell_type":"code","metadata":{"id":"B53_hIq_xGr2","colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"status":"ok","timestamp":1593598895622,"user_tz":-420,"elapsed":4677,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"fad8f022-3fb3-452a-a6a9-398a1a5260c5"},"source":["stop_words = set(stopwords.words('english'))\n","ds['Edit'] = ds['Data'].apply(nltk.word_tokenize)\n","\n","ds['Edit']= ds['Edit'].apply(lambda x: [i for i in x if i not in stop_words])\n","ds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","      <th>Edit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>this was a great movie with a good cast all of...</td>\n","      <td>positive</td>\n","      <td>[great, movie, good, cast, hitting, cylinders]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>even if youre a huge sandler fan please dont b...</td>\n","      <td>negative</td>\n","      <td>[even, youre, huge, sandler, fan, please, dont...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>a movie of outstanding brilliance and a poigna...</td>\n","      <td>positive</td>\n","      <td>[movie, outstanding, brilliance, poignant, unu...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i had the misfortune to watch this rubbish on ...</td>\n","      <td>negative</td>\n","      <td>[misfortune, watch, rubbish, sky, cinema, max,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am at a distinct disadvantage here i have no...</td>\n","      <td>negative</td>\n","      <td>[distinct, disadvantage, seen, first, two, mov...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>this program is a lot of fun and the title son...</td>\n","      <td>positive</td>\n","      <td>[program, lot, fun, title, song, catchy, cant,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  ...                                               Edit\n","0  this was a great movie with a good cast all of...  ...     [great, movie, good, cast, hitting, cylinders]\n","1  even if youre a huge sandler fan please dont b...  ...  [even, youre, huge, sandler, fan, please, dont...\n","2  a movie of outstanding brilliance and a poigna...  ...  [movie, outstanding, brilliance, poignant, unu...\n","3  i had the misfortune to watch this rubbish on ...  ...  [misfortune, watch, rubbish, sky, cinema, max,...\n","4  i am at a distinct disadvantage here i have no...  ...  [distinct, disadvantage, seen, first, two, mov...\n","5  this program is a lot of fun and the title son...  ...  [program, lot, fun, title, song, catchy, cant,...\n","\n","[6 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"LqtNPjovgko1"},"source":["We use tokenize to split the text into smaller pieces because the common way to process the text happens at the token level and we remove stopwords as it have no important meaning. All of these changes are saved into a new column of dataset called edit, so we can see the diffrent after the preprocessing of data."]},{"cell_type":"markdown","metadata":{"id":"0vTW63XYInH3"},"source":["Changing into Base form"]},{"cell_type":"code","metadata":{"id":"d9XZveajImXy","colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"status":"ok","timestamp":1593598895623,"user_tz":-420,"elapsed":4657,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"9f858f56-408a-4229-91df-dc6c5f5d2923"},"source":["\n","#lemmatization\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","ds['Edit'] = ds['Edit'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n","ds\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","      <th>Edit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>this was a great movie with a good cast all of...</td>\n","      <td>positive</td>\n","      <td>[great, movie, good, cast, hitting, cylinder]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>even if youre a huge sandler fan please dont b...</td>\n","      <td>negative</td>\n","      <td>[even, youre, huge, sandler, fan, please, dont...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>a movie of outstanding brilliance and a poigna...</td>\n","      <td>positive</td>\n","      <td>[movie, outstanding, brilliance, poignant, unu...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i had the misfortune to watch this rubbish on ...</td>\n","      <td>negative</td>\n","      <td>[misfortune, watch, rubbish, sky, cinema, max,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am at a distinct disadvantage here i have no...</td>\n","      <td>negative</td>\n","      <td>[distinct, disadvantage, seen, first, two, mov...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>this program is a lot of fun and the title son...</td>\n","      <td>positive</td>\n","      <td>[program, lot, fun, title, song, catchy, cant,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  ...                                               Edit\n","0  this was a great movie with a good cast all of...  ...      [great, movie, good, cast, hitting, cylinder]\n","1  even if youre a huge sandler fan please dont b...  ...  [even, youre, huge, sandler, fan, please, dont...\n","2  a movie of outstanding brilliance and a poigna...  ...  [movie, outstanding, brilliance, poignant, unu...\n","3  i had the misfortune to watch this rubbish on ...  ...  [misfortune, watch, rubbish, sky, cinema, max,...\n","4  i am at a distinct disadvantage here i have no...  ...  [distinct, disadvantage, seen, first, two, mov...\n","5  this program is a lot of fun and the title son...  ...  [program, lot, fun, title, song, catchy, cant,...\n","\n","[6 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"K3aK2nsrhzwZ"},"source":["Using lemmatization we change the input into their base form of word becaise we need to extract the data from the text and insert it into a database."]},{"cell_type":"markdown","metadata":{"id":"5VD4RG4mku50"},"source":["Changing the label form positive and negative into 1 and 0 as it is easier to predict a number."]},{"cell_type":"code","metadata":{"id":"yGR45veu3RfO"},"source":["filter = ds['Label'] == 'positive'\n","dfpos = ds[filter]\n","dfneg = ds[~filter]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRWNFiLr3VM4","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1593598895625,"user_tz":-420,"elapsed":4636,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"080b9aac-2d1a-411e-a21c-1c6831b5bbfc"},"source":["dfpos = dfpos['Edit']\n","dfpos"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        [great, movie, good, cast, hitting, cylinder]\n","2    [movie, outstanding, brilliance, poignant, unu...\n","5    [program, lot, fun, title, song, catchy, cant,...\n","Name: Edit, dtype: object"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"xjndSZ2M5_YY","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1593598895626,"user_tz":-420,"elapsed":4622,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"e9dc076b-5344-4b94-d5a9-b5d816339250"},"source":["dfneg = dfneg['Edit']\n","dfneg"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    [even, youre, huge, sandler, fan, please, dont...\n","3    [misfortune, watch, rubbish, sky, cinema, max,...\n","4    [distinct, disadvantage, seen, first, two, mov...\n","Name: Edit, dtype: object"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"dZTbAzqKkvY2"},"source":["for i in range(len(ds['Label'])) :\n","  if ds['Label'][i] == 'positive':\n","    ds['Label'][i]=1\n","  else :\n","    ds['Label'][i]=0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Cnbg7fQlCPA","colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"status":"ok","timestamp":1593598895628,"user_tz":-420,"elapsed":4598,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"8db84055-649d-4799-fbf1-6401a5970fa8"},"source":["ds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Data</th>\n","      <th>Label</th>\n","      <th>Edit</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>this was a great movie with a good cast all of...</td>\n","      <td>1</td>\n","      <td>[great, movie, good, cast, hitting, cylinder]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>even if youre a huge sandler fan please dont b...</td>\n","      <td>0</td>\n","      <td>[even, youre, huge, sandler, fan, please, dont...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>a movie of outstanding brilliance and a poigna...</td>\n","      <td>1</td>\n","      <td>[movie, outstanding, brilliance, poignant, unu...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>i had the misfortune to watch this rubbish on ...</td>\n","      <td>0</td>\n","      <td>[misfortune, watch, rubbish, sky, cinema, max,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>i am at a distinct disadvantage here i have no...</td>\n","      <td>0</td>\n","      <td>[distinct, disadvantage, seen, first, two, mov...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>this program is a lot of fun and the title son...</td>\n","      <td>1</td>\n","      <td>[program, lot, fun, title, song, catchy, cant,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Data  ...                                               Edit\n","0  this was a great movie with a good cast all of...  ...      [great, movie, good, cast, hitting, cylinder]\n","1  even if youre a huge sandler fan please dont b...  ...  [even, youre, huge, sandler, fan, please, dont...\n","2  a movie of outstanding brilliance and a poigna...  ...  [movie, outstanding, brilliance, poignant, unu...\n","3  i had the misfortune to watch this rubbish on ...  ...  [misfortune, watch, rubbish, sky, cinema, max,...\n","4  i am at a distinct disadvantage here i have no...  ...  [distinct, disadvantage, seen, first, two, mov...\n","5  this program is a lot of fun and the title son...  ...  [program, lot, fun, title, song, catchy, cant,...\n","\n","[6 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"3OeZcUhCksup"},"source":["Taking all of the word and put it into one place"]},{"cell_type":"code","metadata":{"id":"La1cO_ZC61pX"},"source":["def get_all_words(dataset):\n","    allword=[]\n","    for i in dataset:\n","        for y in i:\n","            allword.append(y)\n","    return allword\n","    \n","all_pos_words = get_all_words(dfpos)\n","all_neg_words = get_all_words(dfneg)\n","allword= get_all_words(ds['Edit'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJtmsfVpIfbu","colab":{"base_uri":"https://localhost:8080/","height":887},"executionInfo":{"status":"ok","timestamp":1593598895630,"user_tz":-420,"elapsed":4579,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"5d3f2579-4b74-49c1-9879-6275be2be3df"},"source":["allword"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['great',\n"," 'movie',\n"," 'good',\n"," 'cast',\n"," 'hitting',\n"," 'cylinder',\n"," 'even',\n"," 'youre',\n"," 'huge',\n"," 'sandler',\n"," 'fan',\n"," 'please',\n"," 'dont',\n"," 'bother',\n"," 'extremely',\n"," 'disappointing',\n"," 'comedy',\n"," 'movie',\n"," 'outstanding',\n"," 'brilliance',\n"," 'poignant',\n"," 'unusual',\n"," 'love',\n"," 'story',\n"," 'misfortune',\n"," 'watch',\n"," 'rubbish',\n"," 'sky',\n"," 'cinema',\n"," 'max',\n"," 'cold',\n"," 'winter',\n"," 'night',\n"," 'distinct',\n"," 'disadvantage',\n"," 'seen',\n"," 'first',\n"," 'two',\n"," 'movie',\n"," 'series',\n"," 'program',\n"," 'lot',\n"," 'fun',\n"," 'title',\n"," 'song',\n"," 'catchy',\n"," 'cant',\n"," 'get',\n"," 'head']"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"G7TTSUp5lKYc"},"source":["Preparing the data using word2vec"]},{"cell_type":"code","metadata":{"id":"lges3-XfRLHH","colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"status":"ok","timestamp":1593598895631,"user_tz":-420,"elapsed":4565,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"a497b002-1fca-4ad4-86f5-f312cc4779ea"},"source":["from gensim.models import Word2Vec \n","\n","# Create CBOW model \n","model = Word2Vec([allword], min_count = 1, size = 100, window = 5)\n","#setup the parameter of the model one by1\n","print(model.wv.vocab)\n","print(model.wv.syn0)\n","print(model.wv.syn0.shape)\n","\n","filename=\"word2vecembedding.txt\"\n","model.wv.save_word2vec_format(filename,binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'great': <gensim.models.keyedvectors.Vocab object at 0x7f84456ff898>, 'movie': <gensim.models.keyedvectors.Vocab object at 0x7f84456ff048>, 'good': <gensim.models.keyedvectors.Vocab object at 0x7f84456ffb70>, 'cast': <gensim.models.keyedvectors.Vocab object at 0x7f84456ff5f8>, 'hitting': <gensim.models.keyedvectors.Vocab object at 0x7f84456ffd30>, 'cylinder': <gensim.models.keyedvectors.Vocab object at 0x7f84456ffc18>, 'even': <gensim.models.keyedvectors.Vocab object at 0x7f84456ffba8>, 'youre': <gensim.models.keyedvectors.Vocab object at 0x7f84456ffb38>, 'huge': <gensim.models.keyedvectors.Vocab object at 0x7f844d757d30>, 'sandler': <gensim.models.keyedvectors.Vocab object at 0x7f844d757cc0>, 'fan': <gensim.models.keyedvectors.Vocab object at 0x7f844d7576a0>, 'please': <gensim.models.keyedvectors.Vocab object at 0x7f844d7579e8>, 'dont': <gensim.models.keyedvectors.Vocab object at 0x7f844d757780>, 'bother': <gensim.models.keyedvectors.Vocab object at 0x7f844d757550>, 'extremely': <gensim.models.keyedvectors.Vocab object at 0x7f844d757400>, 'disappointing': <gensim.models.keyedvectors.Vocab object at 0x7f844d757e10>, 'comedy': <gensim.models.keyedvectors.Vocab object at 0x7f844d757128>, 'outstanding': <gensim.models.keyedvectors.Vocab object at 0x7f844d757710>, 'brilliance': <gensim.models.keyedvectors.Vocab object at 0x7f844d757048>, 'poignant': <gensim.models.keyedvectors.Vocab object at 0x7f844d7575c0>, 'unusual': <gensim.models.keyedvectors.Vocab object at 0x7f844d7572b0>, 'love': <gensim.models.keyedvectors.Vocab object at 0x7f844d757c50>, 'story': <gensim.models.keyedvectors.Vocab object at 0x7f844d757668>, 'misfortune': <gensim.models.keyedvectors.Vocab object at 0x7f844d757b00>, 'watch': <gensim.models.keyedvectors.Vocab object at 0x7f844d7576d8>, 'rubbish': <gensim.models.keyedvectors.Vocab object at 0x7f844d757a20>, 'sky': <gensim.models.keyedvectors.Vocab object at 0x7f844d7570b8>, 'cinema': <gensim.models.keyedvectors.Vocab object at 0x7f844d757390>, 'max': <gensim.models.keyedvectors.Vocab object at 0x7f844d7578d0>, 'cold': <gensim.models.keyedvectors.Vocab object at 0x7f844d757978>, 'winter': <gensim.models.keyedvectors.Vocab object at 0x7f844d757080>, 'night': <gensim.models.keyedvectors.Vocab object at 0x7f844d757be0>, 'distinct': <gensim.models.keyedvectors.Vocab object at 0x7f844d757b38>, 'disadvantage': <gensim.models.keyedvectors.Vocab object at 0x7f844d757ac8>, 'seen': <gensim.models.keyedvectors.Vocab object at 0x7f844d757cf8>, 'first': <gensim.models.keyedvectors.Vocab object at 0x7f844d757d68>, 'two': <gensim.models.keyedvectors.Vocab object at 0x7f844d7575f8>, 'series': <gensim.models.keyedvectors.Vocab object at 0x7f844d757160>, 'program': <gensim.models.keyedvectors.Vocab object at 0x7f844d757c18>, 'lot': <gensim.models.keyedvectors.Vocab object at 0x7f844db90f28>, 'fun': <gensim.models.keyedvectors.Vocab object at 0x7f844db90e10>, 'title': <gensim.models.keyedvectors.Vocab object at 0x7f844db904a8>, 'song': <gensim.models.keyedvectors.Vocab object at 0x7f844db90208>, 'catchy': <gensim.models.keyedvectors.Vocab object at 0x7f844db90a20>, 'cant': <gensim.models.keyedvectors.Vocab object at 0x7f844db90eb8>, 'get': <gensim.models.keyedvectors.Vocab object at 0x7f844db90ef0>, 'head': <gensim.models.keyedvectors.Vocab object at 0x7f844db90f98>}\n","[[-0.00056819  0.00457252  0.00498684 ...  0.00389878  0.00338209\n","   0.00266168]\n"," [-0.0006496   0.00219222  0.00494484 ...  0.00037531  0.00435119\n","   0.00488127]\n"," [-0.00033031  0.00071963 -0.00132494 ...  0.00211605  0.00045456\n","   0.00336666]\n"," ...\n"," [-0.00383095 -0.00493367  0.00396371 ... -0.004812   -0.00073539\n","   0.00211927]\n"," [-0.00183481  0.00205217  0.00169385 ...  0.00216567 -0.00326408\n","   0.00143814]\n"," [ 0.00277047  0.00092622  0.00399872 ...  0.00377579 -0.00158465\n","   0.00163788]]\n","(47, 100)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n","  \n","/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1hylv-V2mR6P"},"source":["Word2vec is a two layer neural network that is trained to change word into vector so that the words can be understand by a deep learning model.\n","  From a large corpus of word it will produce a vector space, with each unique  word in a corpus being assign a vector in the space.\n","  Using a neural network word2vec take input as one-hot vectors, which is a vector with the same length, filled with zero excepet the index that represent the word we want to repesent, assign \"1\".\n","\n","the rows of hidden layer weight matrix, are actually the word vector, it operates as a lookup table. The output, is just a word vector of the input word.\n","\n","and the output layer is just a softmax activation function.\n"]},{"cell_type":"code","metadata":{"id":"ahC5PxfoN0GX","colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"status":"ok","timestamp":1593598895632,"user_tz":-420,"elapsed":4550,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"ca4fb909-935d-4c5b-d4dc-b7754c82667c"},"source":["model.wv.most_similar('movie')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('program', 0.18575544655323029),\n"," ('get', 0.14780622720718384),\n"," ('great', 0.13841168582439423),\n"," ('head', 0.1335858404636383),\n"," ('song', 0.1091347187757492),\n"," ('distinct', 0.10216890275478363),\n"," ('rubbish', 0.09900261461734772),\n"," ('two', 0.09880907833576202),\n"," ('huge', 0.09151367098093033),\n"," ('cant', 0.08475174009799957)]"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"KKPFbZRsqrs7"},"source":["After we change the word into vector it become somekind of data of words that's saved into \"word2vecembedding.txt\""]},{"cell_type":"markdown","metadata":{"id":"yTdd8AyOrZDp"},"source":["We need to read the \"word2vecembedding.txt\" and take the value,word, and the index."]},{"cell_type":"code","metadata":{"id":"sKunko1s4PwN","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1627290918874,"user_tz":-420,"elapsed":20,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"e5ba0352-b279-4ab1-c8c3-5848fdb83f75"},"source":["import os\n","embeddings_index = {}\n","f= open(os.path.join('','word2vecembedding.txt'),encoding=\"utf-8\")\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs= np.asarray(values[1:])\n","  embeddings_index[word]=coefs\n","f.close()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-87deb3b1be72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'word2vecembedding.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'word2vecembedding.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"90v0GESNr2i0"},"source":["Using keras we take tokenzier and fit the transformed dataset and using pad_sequences we pad the vector so that all of the input have the same width of vector."]},{"cell_type":"code","metadata":{"id":"g3FcXBA0VDqw","colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"status":"ok","timestamp":1593598895636,"user_tz":-420,"elapsed":4531,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"b46a85fa-df61-47ef-8a1a-5fb20ba72720"},"source":["from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer_obj = Tokenizer()\n","tokenizer_obj.fit_on_texts(ds['Edit'])\n","total_reviews = ds['Edit'].values\n","\n","sequences = tokenizer_obj.texts_to_sequences(ds['Edit'])\n","\n","max_length = max([len(i) for i in total_reviews])\n","print(max_length)\n","\n","\n","#pad\n","word_index= tokenizer_obj.word_index\n","review_pad = pad_sequences(sequences,maxlen=max_length)\n","sentiment = ds['Label'].values\n","print(review_pad)\n","print(sentiment.shape)\n","\n","#define vocab size\n","vocab_size = len(tokenizer_obj.word_index)+1\n","print(vocab_size)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["11\n","[[ 0  0  0  0  0  2  1  3  4  5  6]\n"," [ 7  8  9 10 11 12 13 14 15 16 17]\n"," [ 0  0  0  0  1 18 19 20 21 22 23]\n"," [ 0  0 24 25 26 27 28 29 30 31 32]\n"," [ 0  0  0  0 33 34 35 36 37  1 38]\n"," [ 0  0 39 40 41 42 43 44 45 46 47]]\n","(6,)\n","48\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s73Qkr2JtDe3"},"source":["Form all of the matrix above, we transform the word into vector form the word2vec data by picking the corresponding index. "]},{"cell_type":"code","metadata":{"id":"K_hh2_HduzMX"},"source":["num_words = len(word_index)+1\n","embedding_matrix = np.zeros((num_words,100))\n","for word, i in word_index.items():\n","  if i>num_words:\n","    continue\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6M8UPgNvihk","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593598895639,"user_tz":-420,"elapsed":4508,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"631d5188-5c04-4523-9183-8fe9bb65f369"},"source":["print(num_words)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_dEQTjZyZ1Vu","colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"status":"ok","timestamp":1593598898891,"user_tz":-420,"elapsed":7747,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"68b5cff6-3c4b-4a93-ff0c-184ef39d6043"},"source":["pip install keras_metrics"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras_metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.3.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.18.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.10.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KPQX95P0uCgB"},"source":["These are the BPNN model using keras, in keras the model automaticly use BPNN so we only need to specify the model, the input layer consist of how many the length is and for the hidden layer we use 32 hidden layer. And 1 dense layer of the output."]},{"cell_type":"code","metadata":{"id":"xf3WIXm-e3mn","colab":{"base_uri":"https://localhost:8080/","height":407},"executionInfo":{"status":"ok","timestamp":1593598950918,"user_tz":-420,"elapsed":1080,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"8a3fd0cb-1142-4b38-d9ee-062f788da301"},"source":["from keras import Sequential\n","from keras.layers import Dense,Embedding,LSTM,GRU\n","from keras.layers.embeddings import Embedding\n","import keras_metrics\n","\n","EMBEDDING_DIM = 100\n","\n","model= Sequential()\n","model.add(Embedding(vocab_size,EMBEDDING_DIM,input_length=max_length))\n","model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy',keras_metrics.precision(), keras_metrics.recall(),keras_metrics.f1_score()])\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fp\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fn\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fp\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> tp\n","tracking <tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0> fn\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 11, 100)           4800      \n","_________________________________________________________________\n","gru_4 (GRU)                  (None, 32)                12768     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 17,601\n","Trainable params: 17,601\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MbPr-QOUubyn"},"source":["We split the data into 4 training and 2 testing, so that we can see if the model could project / see if the model could see if the model could predict negative and positive."]},{"cell_type":"code","metadata":{"id":"CHKVgY2Z_pqL","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1593598954329,"user_tz":-420,"elapsed":752,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"5a7e08b3-e9f5-43b4-81a0-7a5dc9d43c31"},"source":["VALIDATION_SPLIT = 0.4\n","indices=np.arange(review_pad.shape[0])\n","\n","review_pad = review_pad[indices]\n","sentiment = sentiment[indices]\n","num_validation_samples = int (VALIDATION_SPLIT*review_pad.shape[0])\n","x_train_pad = review_pad[:-num_validation_samples]\n","y_train = sentiment[:-num_validation_samples]\n","x_test_pad = review_pad[-num_validation_samples:]\n","y_test = sentiment[-num_validation_samples:]\n","\n","print(x_train_pad.shape)\n","print(y_train.shape)\n","print(x_test_pad.shape)\n","print(y_test.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4, 11)\n","(4,)\n","(2, 11)\n","(2,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uBEoMKMavGoI"},"source":["These are the training model"]},{"cell_type":"code","metadata":{"id":"EnRQz_bOx8VT","colab":{"base_uri":"https://localhost:8080/","height":445},"executionInfo":{"status":"ok","timestamp":1593598958772,"user_tz":-420,"elapsed":2971,"user":{"displayName":"Nicholas Evan","photoUrl":"","userId":"11340839917959249724"}},"outputId":"731704d2-cb28-426b-c5ac-59c4b1686439"},"source":["themodel = model.fit(x_train_pad,y_train, epochs= 10, validation_data=(x_test_pad,y_test),verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 4 samples, validate on 2 samples\n","Epoch 1/10\n"," - 1s - loss: 0.6914 - accuracy: 0.5000 - precision: 0.5000 - recall: 1.0000 - f1_score: 0.6667 - val_loss: 0.7085 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 2/10\n"," - 0s - loss: 0.6902 - accuracy: 0.2500 - precision: 0.3333 - recall: 0.5000 - f1_score: 0.4000 - val_loss: 0.7091 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 3/10\n"," - 0s - loss: 0.6823 - accuracy: 0.5000 - precision: 0.5000 - recall: 0.5000 - f1_score: 0.5000 - val_loss: 0.7097 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 4/10\n"," - 0s - loss: 0.6678 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7103 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 5/10\n"," - 0s - loss: 0.6658 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7109 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 6/10\n"," - 0s - loss: 0.6573 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7117 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 7/10\n"," - 0s - loss: 0.6455 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7124 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 8/10\n"," - 0s - loss: 0.6474 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7133 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 9/10\n"," - 0s - loss: 0.6368 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7142 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n","Epoch 10/10\n"," - 0s - loss: 0.6200 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - f1_score: 1.0000 - val_loss: 0.7152 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_score: 0.0000e+00\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u23fuF_jvQNV"},"source":["At the last one, We got accuracy of 1.000(100%) , recall of 1.000(100%) and f1_score of 1.000(100%), This score are good enough, because the dataset only contain of 6 data and from that 6 data there's only 4 data for trainning."]},{"cell_type":"markdown","metadata":{"id":"YLkts7dkFxjk"},"source":["https://medium.com/@ksnugroho/dasar-text-preprocessing-dengan-python-a4fa52608ffe\n","\n","https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n","\n","https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n","\n","https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n","\n","https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n","\n","https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n","\n","https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n","\n"]}]}